## Hugging face

Hugging Face is a machine learning (ML) and data science platform and community that helps users build, deploy and train machine learning models. It provides the infrastructure to demo, run and deploy artificial intelligence (AI) in live applications.

-   Transformers
-   carbon footprint of transformer
-   Transfer Learning
-   Encoder models (eg: seq classification,sentiment analysis , mask modelling)
-   Decoder models (predicting of next word in a sentence, text generation)
-   sequence-to-sequence-model(Encoder-Decoder model): (summarizing texts)

## Tokenizer:

A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a ‚ÄúFast‚Äù implementation based on the Rust library ü§ó Tokenizers.

## Transformers

Transformers provides APIs and tools to easily download and train 
state-of-the-art pretrained models. Using pretrained models can 
reduce your compute costs, carbon footprint, and save you the time
and resources required to train a model from scratch. 
These models support common tasks in different modalities, such as:

üìù Natural Language Processing: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.
üñºÔ∏è Computer Vision: image classification, object detection, and segmentation.
üó£Ô∏è Audio: automatic speech recognition and audio classification.
üêô Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

## neural network

Neural network training is the process of teaching a neural 
network to perform a task. Neural networks learn by initially 
processing several large sets of labeled or unlabeled data.
By using these examples,
they can then process unknown inputs more accurately.

## Gradient function

Gradient with PyTorch - javatpoint
The gradient is used to find the derivatives of the function.
In mathematical terms, derivatives mean differentiation of a 
function partially and finding the value. Below is the diagram of how
to calculate the derivative of a function.

## Pytorch

Pytorch is an open source deep learning framework which is predominantly used
in python. it is used quite heavily in state-of the art model as well as in
deep learning research. 

## Pytorch Tensor:

A PyTorch Tensor is basically the same as a numpy array:
it does not know anything about deep learning or computational
graphs or gradients, and is just a generic n-dimensional array
to be used for arbitrary numeric computation.

## Directed Acyclic graph:

A DAG is a Directed Acyclic Graph, a type of graph whose nodes are 
directionally related to each other and don't form a directional
closed loop. In the practice of analytics engineering, DAGs are
often used to visually represent the relationships between your
data models.